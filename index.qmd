---
title: "Bayesian Unobserved Component Models"
author:
  - name: Tomasz WoÅºniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
    name: Stephan Berke
    url: https://github.com/sb955/
    orcid: 0009-0004-8443-1453
  
    execute:
  #echo: false
citation: 
  issued: 2024-05-01
  url: https://donotdespair.github.io/Bayesian-Unobserved-Component-Models/
  doi: 10.26188/25814617
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. 
> The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. 
> We scrutinise Bayesian forecasting and sampling from the predictive density.
>
> **Keywords.** Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling

# Unobserved component models

Unobserved Component (UC) models are a popular class of models in macroeconometrics that use the state-space representation for unit-root nonstationary time series. 
The simple formulation of the model equations decomposing the series into a non-stationary and stationary component facilitates economic interpretations and good forecasting performance.

# A simple local-level model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. 
It decomposes the variable into a stochastic trend component, $\tau_t$, and a stationary error component, $\epsilon_t$. 
The former follows a Gaussian random walk process with the conditional variance $\sigma_\eta^2$, and the latter is zero-mean normally distributed with the variance $\sigma^2$.
These are expressed as the model equations:
\begin{align}
y_t &= \tau_t + \epsilon_t,\\
\tau_t &= \tau_{t-1} + \eta_t,\\
\epsilon_t &\sim\mathcal{N}\left(0, \sigma^2\right),\\
\eta_t &\sim\mathcal{N}\left(0, \sigma_\eta^2\right),
\end{align}
where the initial condition $\tau_0$ is a parameter of the model.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. 
Define a $T$-vector of zeros, $\mathbf{0}_T$, and of ones, $\boldsymbol\imath_T$, the identity matrix of order $T$, $\mathbf{I}_T$, as well as $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots\\ y_T \end{bmatrix},\quad
\boldsymbol\tau = \begin{bmatrix} \tau_1\\ \vdots\\ \tau_T \end{bmatrix},\quad
\boldsymbol\epsilon = \begin{bmatrix} \epsilon_1\\ \vdots\\ \epsilon_T \end{bmatrix},\quad
\boldsymbol\eta = \begin{bmatrix} \eta_1\\ \vdots\\ \eta_T \end{bmatrix},\qquad
\mathbf{i} = \begin{bmatrix} 1\\0\\ \vdots\\ 0 \end{bmatrix},
\end{align}
and a $T\times T$ matrix $\mathbf{H}$ with the elements:
\begin{align}
\mathbf{H} = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
-1 & 1 & \cdots & 0 & 0\\
0 & -1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0\\
0 & 0 & \cdots & -1 & 1
\end{bmatrix}.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{\tau} + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\epsilon &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right),\\
\boldsymbol\eta &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma_\eta^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\boldsymbol\epsilon$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \boldsymbol\tau, \sigma^2 &\sim\mathcal{N}_T\left(\boldsymbol\tau, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y})\equiv p\left(\mathbf{y}\mid\boldsymbol\tau, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of $\mathbf{y}$, is considered a function of parameters $\boldsymbol\tau$ and $\sigma^2$ is given by: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \boldsymbol\tau)'(\mathbf{y} - \boldsymbol\tau)\right\}.
\end{align}

## Prior distributions

# Bayesian estimation

## Gibbs sampler

## Simulation smoother and precision sampler

## Analytical solution for a joint posterior

# Hierarchical modeling

## Estimating gamma error term variance prior scale


## Estimating inverted-gamma 2 error term variance prior scale

## Estimating the initial condition prior scale

## Student-t prior for the trend component

## Estimating Student-t degrees of freedom parameter

## Laplace prior for the trend component

# Model extensions

## Estimation of autoregressive parameters for cycle component

## Autoregressive cycle component

## Random walk with  time-varying drift parameter

## Student-t error terms

This section proposes an approach to implement t-Distributed error terms in unobserved component models by following the methodology of Chan(2020). 

$$
\epsilon\sim N_{T}(0_{T},{\sigma^{2}}diag({\lambda_{t})})
$$

The model sepeartes the cross sectional and serial covariance, $\sigma^{2}$ and $\lambda_{t}$.
If each $\lambda_{t}$ follows independently an $IG2\sim(\nu,\nu)$, then marginally $\epsilon_{t}$ has a t-distribution. 
The kernel of the likelihood is given by: 

$$
L(y|\tau,\sigma^{2},\lambda_{t}) = (\sigma^2)^{\frac{T}{2}}(\lambda_{t})^{-\frac{_{N}}{2}}
exp\{-\frac{1}{2}\frac{1}{\sigma^{2}}\frac{1}{\lambda_{t}}(y-\tau)'(y-(\tau)\}.
$$

In this framework, the parameters $\tau$ and $\sigma^{2}$ follow a normal inverse gamma 2 distribution 

$$p(\tau|\sigma^{2})\sim N(\underline{\tau}, \underline{\sigma_{\tau}^{2}})$$

$$p(\sigma^{2})\sim IG2({\underline{s}, {\underline{{\nu}}})}$$

where 
$$
p(\tau|\sigma^{2}) = \exp \left\{-\frac{1}{2} \frac{1}{\sigma^{2}} \frac{1}{\underline{\sigma_{\tau}^{2}}} (\tau - \underline{\tau})' (\tau - \underline{\tau})\right\}
$$
$$
p(\sigma^{2})=(\sigma^{2})^{{\underline{\tau}+ N}}exp\{-\frac{1}{2}\frac{{\underline{s}}}{\sigma^{2}}\}
$$

Combining the likelihood and the priors 

```{=tex}
\begin{align}
p(\tau,\sigma^{2}|y) &\propto L(y|\tau,\sigma^{2},\lambda_{t}) \times p(\tau|\sigma^{2}) \times p(\sigma^{2})

&=(\sigma^{2})^{T/2}(\lambda_{t})^{-\frac{N}{2}}exp\{-\frac{1}{2}\frac{1}{\sigma^{2}}\frac{1}{\lambda_{t}}(y-\tau)'(y-\tau)\}

&\times exp\{-\frac{1}{2}\frac{1}{\sigma^{2}}\frac{1}{\underline{\sigma_{\tau}^{2}}}(\tau-\underline{\tau})'(\tau-\underline{\tau})\}

&\times (\sigma^{2})^{{\underline{\tau}+ N}}exp\{-\frac{1}{2}\frac{\underline{s}}{\sigma^{2}}\}
\end{align}
```

After rearranging terms, the posterior function can be obtained as: 

```{=tex}
\begin{align}
p(\tau,\sigma^{2}|y) &=(\sigma^{2})^{^{\frac{N+K+\text{{\bar{\tau}}+1}}{2}}}(\lambda_{t})^{-\frac{_{N}}{2}}

&\times exp\{-\frac{1}{2}\frac{1}{\bar{\sigma_{\tau}^{2}}}(\tau-\bar{\tau})'(\tau-\bar{\tau})\}

&\times exp\{-\frac{1}{2}\frac{1}{\sigma^{2}}\bar{s}\}
\end{align}
```

The posterior follows a normal inverse wishart distribution

$$
p(\tau, \sigma^{2} | y) \sim NIW(\bar{\tau}, \bar{\sigma}^{2}, \bar{\nu}, \bar{s})
$$

With the posterior parameters represented as: 

$$
\bar{\sigma}_{\tau}^{2}=(\frac{1}{\lambda_{t}}X'X+{\underline{\sigma}_{\tau}^{2}})^{-1}
$$
$$
\bar{\tau}=\bar{\sigma}^{2}(\frac{1}{\lambda_{t}}X'Y+\underline{\sigma}_{\tau}^{2} \underline{\tau})
$$
$$
\bar{\nu}=T+{\underline{\tau}}
$$

$$
\bar{s}= \underline{s}+ \frac{1}{\lambda_{t}}Y'Y+ \underline{\tau}'{\underline{\sigma}_{\tau}^{2}\underline{\tau}}
$$

###Distribution of $\lambda_{t}$

The prior density of $\lambda_{t}\sim IG2(\nu_{\lambda},\nu_{\lambda})$ defines as

$$
p(\lambda_{t}|\nu_{\lambda})=\lambda_{t}^{-\frac{\nu_{\lambda}+2}{2}}\times exp\{-\frac{1}{2}\frac{1}{\lambda_{t}}\nu_{\lambda}\}.
$$

Combining the likelihood and the prior 

```{=tex}
\begin{align}
p(\lambda_{t}|y,\tau,\sigma^{2})\propto L(\lambda_{t},\tau,\sigma^{2}|y)*p(\lambda_{t}|\nu_{\lambda})
&= (\lambda_{t})^{-\frac{_{N}}{2}}exp\{-\frac{1}{2}\frac{1}{\sigma^{2}}\frac{1}{\lambda_{t}}(y-\tau)'(y-(\tau)\}
&\times \lambda_{t}^{-\frac{\nu_{\lambda}+2}{2}}\times exp\{-\frac{1}{2}\frac{1}{\lambda_{t}}\nu_{\lambda}\}
\end{align}
```
Yields the posterior function

$$
p(\lambda_{t}|y,\tau,\sigma^{2})=(\lambda_{t})^{-\frac{_{N+\nu+2}}{2}}exp\{-\frac{1}{2}\frac{1}{\lambda_{t}}\epsilon'\frac{1}{\sigma^{2}}\epsilon\}
$$

where $\epsilon_{t}=y_{t}-\tau_{t}$

It can be observed. that the kernel of the posterior follows an inverse gamma 2 distribution

$$IG2\sim(N+\nu_{\lambda},\nu_{\lambda}+\epsilon_{t}'\frac{1}{\sigma^{2}}\epsilon_{t})$$

Sampling of $\nu_{\lambda}$

Using the Metropolis-Hastings algorithm, one can sample at each iteration for 

$$\nu_{\lambda}\sim N(\nu^{s-1},\sigma_{\nu}^{2})$$

Sampling algorithm

1. Draw ($\sigma^{2})^{(s)}$ from $IW(\bar{\nu},\bar{s})$

2. Draw $\tau^{(s)}$ from $N(\bar{\tau},(\sigma^{2})^{(s)},\bar{{\sigma_{\tau}^{2}}})$

3. Draw $\lambda_{t}$ from $IG2\sim(N+\nu_{\lambda},\nu_{\lambda}+\epsilon_{t}'\frac{1}{\sigma^{2}}\epsilon_{t})$

4. Draw $\nu^{*(s)}$ from $N(\nu^{s-1},\sigma_{\nu}^{2})$

```{r}
library(MCMCpack)
library(mvtnorm)
library(RcppTN)

posterior_t <- function(Y, X, p, S){
  
  #p=1
  #S=1000
  
  A.hat       = solve(t(X)%*%X)%*%t(X)%*%Y
  Sigma.hat   = t(Y-X%*%A.hat)%*%(Y-X%*%A.hat)/nrow(Y)
  
  N = ncol(Y)
  t <- NROW(Y)
  kappa.1   = 1
  kappa.2   = 100
  K = 1 + (p*N)
  
  A.prior     = matrix(0, K , N)
  A.prior[2:(N+1),] = diag(N)
  V.prior     = diag(c(kappa.2,kappa.1*((1:p)^(-2))%x%rep(1,N)))
  S.prior     = diag(diag(Sigma.hat))
  nu.prior    = N+1
  lambda.nu.prior = 5
  
  
  
  lambda.0 = rinvgamma(t, lambda.nu.prior/2, lambda.nu.prior/2)
  
  
  Sigma.posterior.draws <- array(dim = c(N, N, S))
  A.posterior.draws <- array(dim = c(K, N, S))
  lambda.posterior.draws <- array(NA,c(t,S))
  
  
  for (s in 1:S){
    if (s == 1) {
      lambda.s = lambda.0
    } else {
      lambda.s    = lambda.posterior.draws[,s-1]
    }
    Omega = (diag(lambda.s))
    Omega.inv = diag(1/lambda.s)
    
    V.bar.ext       = solve(t(X)%*% Omega.inv%*%X + solve(V.prior))
    A.bar.ext       = V.bar.ext%*%(t(X)%*% Omega.inv%*%Y + solve(V.prior)%*%A.prior)
    nu.bar.ext      = t + nu.prior
    S.bar.ext       = S.prior + t(Y)%*% Omega.inv%*%Y + t(A.prior)%*%solve(V.prior)%*%A.prior - t(A.bar.ext)%*%solve(V.bar.ext)%*%A.bar.ext
    S.bar.ext.inv   = solve(S.bar.ext)
    
    Sigma.inv.draw = rWishart(1, nu.bar.ext, S.bar.ext.inv)[,,1]
    Sigma.posterior.draws[,,s] = solve(Sigma.inv.draw)
    A.posterior.draws[,,s] = matrix(mvtnorm::rmvnorm(1, mean=as.vector(A.bar.ext), sigma = Sigma.posterior.draws[,,s] %x% V.bar.ext), ncol=N)
    
    for (x in 1:t) {
    u_t <- Y.bv[x,] - t(X.bv[x,]) %*% A.posterior.draws[,,s]
    lambda.posterior.draws[x, s] <- rinvgamma(1, (N + lambda.nu[s])/2, (lambda.nu[s] + u_t %*% Sigma.posterior.draws[,,s] %*% t(u_t))/2)
    }
  
    if (s>=2){
    
    #sample nu from a truncated normal distribution
    lambda.nu.star <- RcppTN::rtn(1, .mean = lambda.nu[s-1], .sd = var.lambda.nu, .low = 0, .high = Inf)
    
    #densities of proposed value
    log_likelihood_star<- 1/sum((lambda.nu.star/2) * log(lambda.nu.star/2) - lgamma(lambda.nu.star/2) - (lambda.nu.star/2 + 1) * log(lambda.posterior.draws[,s]) - (lambda.nu.star/2) / lambda.posterior.draws[,s] )
    #or simply write with function
    #log_likelihood_star <- 1/(sum(dgamma(lambda.posterior.draws[,s], shape = lambda.nu.star/2, rate = lambda.nu.star/2, log = TRUE)))

    #prior distribution of nu of proposed value
    log_prior_nu_star <- dnorm(lambda.nu.star, mean = lambda.nu[s-1], sd = var.lambda.nu, log = TRUE)
    
    #truncated prior of nu of proposed value
    log_prior_nu_trunc_star <- log(RcppTN::dtn(lambda.nu.star, .mean = lambda.nu[s-1], .sd = var.lambda.nu))
    
    
    
    #densities of current value
    log_likelihood_curr<- 1/sum((lambda.nu[s-1]/2) * log(lambda.nu[s-1]/2) - lgamma(lambda.nu[s-1]/2) - (lambda.nu[s-1]/2 + 1) * log(lambda.posterior.draws[,s]) - (lambda.nu[s-1]/2) / lambda.posterior.draws[,s] )
    #log_likelihood_curr <- 1/(sum(dgamma(lambda.posterior.draws[,s], shape = lambda.nu[s-1]/2, rate = lambda.nu[s-1]/2, log = TRUE)))
    
    #prior distribution of nu of current value
    log_prior_nu_curr <- dnorm(lambda.nu[s-1], mean = lambda.nu[s-1], sd = var.lambda.nu, log = TRUE)
    #truncated prior of nu of current value
    log_prior_nu_trunc_curr <- log(RcppTN::dtn(lambda.nu[s-1], .mean = lambda.nu[s-1], .sd = var.lambda.nu))
    
    
    #calculate ratio 
    log_ratio <- (log_likelihood_star + log_prior_nu_star + log_prior_nu_trunc_star) - (log_likelihood_curr + log_prior_nu_curr+ log_prior_nu_trunc_curr)
    
    ratio[s] <- exp(log_ratio)
    
    alpha[s] <- min(1, ratio[s])
    
    if (runif(1) < alpha[s]) {
      lambda.nu[s] <- lambda.nu.star
    } else{
      lambda.nu[s] <- lambda.nu[s-1]
    }
    
  }  
  pris = (list(A.posterior.draws = A.posterior.draws, 
               Sigma.posterior.draws = Sigma.posterior.draws,
               lambda.posterior.draws=lambda.posterior.draws,
               lambda.nu=lambda.nu))

  }
}

# median.lambda.nu <- median(lambda.nu)
# # Plot the nu draws
# plot(lambda.nu, type = 'l', col = 'black', lwd = 2, 
#      main = expression(paste(lambda_nu, " draws")), 
#      xlab = "Iteration", ylab = expression(lambda_nu))
# abline(h = median.lambda.nu, col = 'blue', lwd = 1)
# 
# library(coda)
# print(1-rejectionRate(as.mcmc(lambda.nu)))

```


### Reference

Chan, Joshua CC. 2020. âLarge Bayesian VARs: A Flexible Kronecker Error Covariance Structure.â Journal of Business & Economic Statistics 38 (1): 68â79.


## Conditional heteroskedasticity

# Bayesian forecasting

## Predictive density

## Sampling from the predictive density




## References {.unnumbered}
