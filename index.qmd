---
title: "Bayesian Unobserved Component Models"
author:
  - name: Tomasz WoÅºniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378

  - name: Ben Gussen
    url: https://github.com/bgussen
    orcid: 0000-0003-4406-4076
  - name: Pun Suparattanapinan
    url: https://github.com/Punzbd


  
    execute:
  #echo: false
citation: 
  issued: 2024-05-01
  url: https://donotdespair.github.io/Bayesian-Unobserved-Component-Models/
  doi: 10.26188/25814617
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. 
> The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. 
> We scrutinise Bayesian forecasting and sampling from the predictive density.
>
> **Keywords.** Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling

# Unobserved component models

Unobserved Component (UC) models are a popular class of models in macroeconometrics that use the state-space representation for unit-root nonstationary time series. 
The simple formulation of the model equations decomposing the series into a non-stationary and stationary component facilitates economic interpretations and good forecasting performance.

# A simple local-level model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. 
It decomposes the variable into a stochastic trend component, $\tau_t$, and a stationary error component, $\epsilon_t$. 
The former follows a Gaussian random walk process with the conditional variance $\sigma_\eta^2$, and the latter is zero-mean normally distributed with the variance $\sigma^2$.
These are expressed as the model equations:
\begin{align}
y_t &= \tau_t + \epsilon_t,\\
\tau_t &= \tau_{t-1} + \eta_t,\\
\epsilon_t &\sim\mathcal{N}\left(0, \sigma^2\right),\\
\eta_t &\sim\mathcal{N}\left(0, \sigma_\eta^2\right),
\end{align}
where the initial condition $\tau_0$ is a parameter of the model.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. 
Define a $T$-vector of zeros, $\mathbf{0}_T$, and of ones, $\boldsymbol\imath_T$, the identity matrix of order $T$, $\mathbf{I}_T$, as well as $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots\\ y_T \end{bmatrix},\quad
\boldsymbol\tau = \begin{bmatrix} \tau_1\\ \vdots\\ \tau_T \end{bmatrix},\quad
\boldsymbol\epsilon = \begin{bmatrix} \epsilon_1\\ \vdots\\ \epsilon_T \end{bmatrix},\quad
\boldsymbol\eta = \begin{bmatrix} \eta_1\\ \vdots\\ \eta_T \end{bmatrix},\qquad
\mathbf{i} = \begin{bmatrix} 1\\0\\ \vdots\\ 0 \end{bmatrix},
\end{align}
and a $T\times T$ matrix $\mathbf{H}$ with the elements:
\begin{align}
\mathbf{H} = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
-1 & 1 & \cdots & 0 & 0\\
0 & -1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0\\
0 & 0 & \cdots & -1 & 1
\end{bmatrix}.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{\tau} + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\epsilon &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right),\\
\boldsymbol\eta &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma_\eta^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\boldsymbol\epsilon$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \boldsymbol\tau, \sigma^2 &\sim\mathcal{N}_T\left(\boldsymbol\tau, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y})\equiv p\left(\mathbf{y}\mid\boldsymbol\tau, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of $\mathbf{y}$, is considered a function of parameters $\boldsymbol\tau$ and $\sigma^2$ is given by: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \boldsymbol\tau)'(\mathbf{y} - \boldsymbol\tau)\right\}.
\end{align}

## Prior distributions

# Bayesian estimation

## Gibbs sampler

## Simulation smoother and precision sampler

## Analytical solution for a joint posterior

# Hierarchical modeling

## Estimating gamma error term variance prior scale


## Estimating inverted-gamma 2 error term variance prior scale

## Estimating the initial condition prior scale

## Student-t prior for the trend component

## Estimating Student-t degrees of freedom parameter

## Laplace prior for the trend component

# Model extensions

## Estimation of autoregressive parameters for cycle component

## Autoregressive cycle component

In this section, we will modify the error component $\epsilon_{t}$ from the simple local-level model to take the form of an autoregressive (AR) expression as follows.
$$
\alpha_{p}(L)\epsilon_{t} = e_{t} \\ 
$$
$$
e_{t}\sim \mathcal{N}(0,\sigma_{e}^{2})
$$

where $\alpha_{p}(L)$ is a lag polynomial, which can be represented as

$$
\alpha_{p}(L) = 1- \alpha_{1}L - \alpha_{2}L^{2} - \cdots  - \alpha_{p}L^{p}
$$

Then, the error component will be obtained in the form of an autoregressive model, represented as 

$$
\epsilon_{t} = \alpha_{1}\epsilon_{t-1} + \alpha_{2}\epsilon_{t-2} + \cdots  + \alpha_{p}\epsilon_{t-p} + e_{t}
$$
And a vector of autoregressive parameters, denoted as $\boldsymbol\alpha = (\alpha_{1},\alpha_{2}, \cdots, \alpha_{p})'$. Where $\boldsymbol\alpha \in A$, with the stationarity restriction being required to hold.

By modeling the error component in an autoregressive form, we enable our model to capture the cyclical component, which may deviate in the short run from the long-term stochastic trend ($\tau_{t}$).

**The matrix notation can be represented as:**

```{=tex}
\begin{align}
\mathbf{H}_{\alpha} \boldsymbol\epsilon &= \mathbf{e} \\
\boldsymbol\epsilon &= H_{\alpha}^{-1}\mathbf{e} \\
\mathbf{e} &\sim \mathcal{N}_{T}(\mathbf{0}_{T},\sigma_{e}^{2} \mathbf{I}_{T})
\end{align}
```


Where 

$$ 
\mathbf{H}_{\alpha} =
\begin{bmatrix}
 1  &0  &0  &0  &0  &\cdots  &0  &0  &0  &0 \\
 -\alpha_{1}  &1  &0  &0  &0  &\cdots  &0  &0  &0  &0 \\
 -\alpha_{2}  &-\alpha_{1}  &1  &0  &0  &\cdots  &\vdots  &\vdots  &\vdots  &\vdots \\
 -\alpha_{3}  &-\alpha_{2}  &-\alpha_{1}  &1  &0  &\ddots  &\vdots  &\vdots  &\vdots  &\vdots \\
 \vdots   &\vdots  &\ddots  &\ddots  & \ddots &\ddots  &\vdots  &\vdots  &\vdots &\vdots \\
 -\alpha_{p}  &-\alpha_{p-1}  &-\alpha_{p-2} &\ddots  &\ddots  &\ddots  &0  &0  &0  &0 \\
 \vdots  &\vdots  &\vdots  &\vdots  &\ddots  &\ddots  &\ddots  &\vdots  &\vdots &\vdots  \\
 \vdots  &\vdots  &\vdots  &\vdots  &\cdots  &\ddots  &-\alpha_{1}  &1  &0  &0 \\
 0  &0  &0  &0  &\cdots  &\cdots   &-\alpha_{2}   &-\alpha_{1} &1 &0 \\
 0  &0  & 0 &0  &\cdots  &\cdots  &-\alpha_{3}  &-\alpha_{2}  &-\alpha_{1} &1 
\end{bmatrix}_{TXT} 
$$

```{=tex}
\begin{align}
\boldsymbol\epsilon =
\begin{bmatrix}
\epsilon_{1} \\
\epsilon_{2}\\
\vdots\\
\epsilon_{T}\\
\end{bmatrix}_{TX1} 

\;\;\;\;\;\;\;\;\;\;\; and \;\;\;\;\;\;\;\;\;\;\;
\mathbf{e} =
\begin{bmatrix}
e_{1} \\
e_{2}\\
\vdots\\
e_{T}\\
\end{bmatrix}_{TX1} 

\end{align}
```


<br>

Typically, in most cases, we set the initial value ($\boldsymbol\epsilon_{0}$) to zero since it aligns with the well-defined stationarity assumption. However, in this instance, the author will also estimate $\boldsymbol\epsilon_{0}$ from an autoregressive model, following a normal prior distribution: $\boldsymbol\epsilon_{0} \sim \mathcal{N}_{p}(\mathbf{0}_{P},\sigma_{0}^{2}\mathbf{I}_{P})$. It is also assumed that $\mathbf{e}_{0} \sim \mathcal{N}_{P}(\mathbf{0}_{P},\sigma_{e}^{2}\mathbf{I}_{P})$.

Which can be represented in matrix form as:

```{=tex}
\begin{align}
\boldsymbol\epsilon_{0} =
\begin{bmatrix}
\epsilon_{-p+1}   \\
\epsilon_{-p+2}   \\
\vdots\\
\epsilon_{0} 
\end{bmatrix}_{PX1} 

\;\;\;\;\;\;\;\;\;\;\; and \;\;\;\;\;\;\;\;\;\;\;
\mathbf{e}_{0} = 
\begin{bmatrix}
e_{-p+1}   \\
e_{-p+2}   \\
\vdots\\
e_{0} 
\end{bmatrix}_{PX1} 

\end{align}
```


Then, the author will combine $\boldsymbol\epsilon$ and $\boldsymbol\epsilon_{0}$ together. This can be represented using the notations $\mathbf{H}_{\alpha}^{*}$, $\boldsymbol\epsilon^{*}$, and $\mathbf{e}^{*}$.

$\mathbf{H}_{\alpha}^{*}$ is constructed in the same way as $\mathbf{H}_{\alpha}$, but the dimension increases to $(T+P)X(T+P)$.

```{=tex}
\begin{align}
\boldsymbol\epsilon^{*} =
\begin{bmatrix}
\epsilon_{0}   \\
\epsilon
\end{bmatrix}_{(T+P)X1} 

\;\;\;

 =  \;\;\;
\begin{bmatrix}
\epsilon_{-p+1}   \\
\epsilon_{-p+2}   \\
\vdots\\
\epsilon_{1} \\
\epsilon_{2} \\
\vdots\\
\epsilon_{T}
\end{bmatrix}_{(T+P)X1}

\;\;\;\; and \;\;\;\;

\mathbf{e}^{*} =
\begin{bmatrix}
e_{0}   \\
e
\end{bmatrix}_{(T+P)X1} 

\;\;\;

 =  \;\;\;
\begin{bmatrix}
e_{-p+1}   \\
e_{-p+2}   \\
\vdots\\
e_{1} \\
e_{2} \\
\vdots\\
e_{T}
\end{bmatrix}_{(T+P)X1}

\end{align}
```

**The matrix notation can be represented as:**
```{=tex}
\begin{align}
\mathbf{H}_{\alpha}^{*} \boldsymbol\epsilon^{*} &=\mathbf{e}^{*} \\
\boldsymbol\epsilon^{*} &= \mathbf{H}^{*-1}_{\alpha}\mathbf{e}^{*} \\

\mathbf{e}^{*} &\sim \mathcal{N}_{T+P}(\mathbf{0}_{T+P},\sigma_{e*}^{2}\mathbf{I}_{T+P})
\end{align}
```

Then
```{=tex}
\begin{align}

\boldsymbol\epsilon^{*}|\boldsymbol\alpha,\sigma_{e}^2 &\sim \mathcal{N}_{T+P}(\mathbf{0}_{T+P},\underbrace{\sigma_{e*}^2(\mathbf{H}_{\alpha}^{*'}\mathbf{H}_{\alpha}^{*})^{-1}}_{{\boldsymbol\Omega}})
\end{align}
```


```{=tex}
\begin{align}
\boldsymbol\Omega = \begin{bmatrix}
 \boldsymbol\Omega_{11}& \boldsymbol\Omega_{12}\\
 \boldsymbol\Omega_{21}& \boldsymbol\Omega_{21}
\end{bmatrix} \\
\end{align}

\text{ with dimension}

 \begin{bmatrix}
 PXP&PXT\\
 TXP&TXT
\end{bmatrix}_{(T+P)X(T+P)} \\

```

### The prior distribution

Using knowledge from the conditional distributions of multivariate normal distribution, we can find the prior distribution of $\boldsymbol\epsilon \text{ and } \boldsymbol\epsilon_{0}$ as follows.

1. The prior distribution of $\boldsymbol\epsilon$

```{=tex}
\begin{align}

\boldsymbol\epsilon|\boldsymbol\epsilon_{0},\boldsymbol\alpha,\sigma_{e*}^2 &\sim \mathcal{N}_{T}(\underbrace{\boldsymbol\Omega_{21}\boldsymbol\Omega_{11}^{-1}\boldsymbol\epsilon_{0}}_{\underline{\boldsymbol\epsilon}},\underbrace{\boldsymbol\Omega_{22}-\boldsymbol\Omega_{21}\boldsymbol\Omega_{11}^{-1}\boldsymbol\Omega_{12}}_{\underline{\mathbf{V}}_{\epsilon}}) \\
&\propto exp\left\{-\frac{1}{2} (\boldsymbol\epsilon-\underline{\boldsymbol\epsilon})' \underline{\mathbf{V}}_{\epsilon}^{-1} (\boldsymbol\epsilon-\underline{\boldsymbol\epsilon})\right\}


\end{align}
```

2. The prior distribution of $\boldsymbol\epsilon_{0}$

```{=tex}
\begin{align}

\boldsymbol\epsilon_{0}|\boldsymbol\epsilon,\boldsymbol\alpha,\sigma_{e*}^2 &\sim \mathcal{N}_{P}(\underbrace{\boldsymbol\Omega_{12}\boldsymbol\Omega_{22}^{-1}\boldsymbol\epsilon}_{\underline{\boldsymbol\epsilon}_{0}},\underbrace{\boldsymbol\Omega_{11}-\boldsymbol\Omega_{12}\boldsymbol\Omega_{22}^{-1}\boldsymbol\Omega_{21}}_{\underline{\mathbf{V}}_{\epsilon_{0}}}) \\

&\propto exp\left\{-\frac{1}{2} (\boldsymbol\epsilon_{0}-\underline{\boldsymbol\epsilon}_{0})' \underline{\mathbf{V}}_{\epsilon_{0}}^{-1}  (\boldsymbol\epsilon_{0}-\underline{\boldsymbol\epsilon}_{0})\right\}


\end{align}
```


### Likelihood
From the simple local-level model we can write

```{=tex}
\begin{align}
\mathbf{y} &= \boldsymbol\tau + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\tau &= \mathbf{H}^{-1} \mathbf{i} \tau_0 + \mathbf{H}^{-1} \boldsymbol\eta
\end{align}
```


Then,
```{=tex}
\begin{align}
\boldsymbol\epsilon &= \mathbf{y} - \boldsymbol\tau \\
\boldsymbol\epsilon &\sim N(\mathbf{y}-\mathbf{H}^{-1} \mathbf{i} \tau_0, \sigma_{\eta}^2 \mathbf{I}_{T} (\mathbf{H}'\mathbf{H})^{-1})\\
L(\boldsymbol\epsilon|\mathbf{y},\boldsymbol\tau,\tau_0, \sigma_{\eta}^2)  &\propto exp\{-\frac{1}{2}\frac{1}{\sigma_{\eta}^2}(\boldsymbol\epsilon-(\mathbf{y}-\mathbf{H}^{-1} \mathbf{i} \tau_0))' \mathbf{H}'\mathbf{H} (\boldsymbol\epsilon-(\mathbf{y}-\mathbf{H}^{-1} \mathbf{i} \tau_0))
\end{align}
```

### The full-conditional posterior distribution

1. The full-conditional posterior distribution of $\boldsymbol\epsilon$

```{=tex}
\begin{align}
p(\boldsymbol\epsilon|\boldsymbol\epsilon_{0},\mathbf{y},\boldsymbol\tau,\tau_0, \sigma_{\eta}^2, \underline{\mathbf{V}}_{\epsilon}) &\propto L(\boldsymbol\epsilon|\mathbf{y},\boldsymbol\tau,\tau_0,\sigma_{\eta}^2) \; p(\boldsymbol\epsilon|\boldsymbol\epsilon_{0},\boldsymbol\alpha,\sigma_{e*}^2) \\
&= \mathcal{N}_{T}(\bar{\boldsymbol\epsilon},\bar{\mathbf{V}}_{\epsilon}) \\

\\

\bar{\mathbf{V}}_{\epsilon} &= \left[\underline{\mathbf{V}}^{-1}_{\epsilon} + \sigma_{\eta}^{-2}\mathbf{H}'\mathbf{H}\right]^{-1} \\
\bar{\boldsymbol\epsilon} &= \bar{\mathbf{V}}_{\epsilon} \left[\sigma_{\eta}^{-2} \mathbf{H}'\mathbf{H} (\mathbf{y}-\mathbf{H}^{-1} \mathbf{i} \tau_0) + \underline{\mathbf{V}}^{-1}_{\epsilon}\; \underline{\boldsymbol\epsilon}\right]

\end{align}
```

2. The full-conditional posterior distribution of $\boldsymbol\epsilon_{0}$


```{=tex}
\begin{align}

p(\boldsymbol\epsilon_{0}|\epsilon,\mathbf{y},\boldsymbol\tau,\tau_0, \sigma_{\eta}^2,\underline{\mathbf{V}}_{\epsilon_{0}}) &\propto L(\boldsymbol\epsilon|\mathbf{y},\boldsymbol\tau,\tau_0,\sigma_{\eta}^2) \; p(\boldsymbol\epsilon_{0}) \; p(\boldsymbol\epsilon_{0}|\boldsymbol\epsilon,\boldsymbol\alpha,\sigma_{e*}^2) \\


&\propto p(\boldsymbol\epsilon_{0}) \; p(\boldsymbol\epsilon_{0}|\boldsymbol\epsilon,\boldsymbol\alpha,\sigma_{e*}^2) \\

&= \mathcal{N}_{P}(\bar{\boldsymbol\epsilon_{0}},\bar{\mathbf{V}}_{\epsilon_{0}}) \\

\\

\bar{\mathbf{V}}_{\epsilon_{0}} &= \left[\underline{\mathbf{V}}^{-1}_{\epsilon_{0}} + \sigma_{0}^{-2} \mathbf{I}_{P}\right]^{-1} \\
\bar{\boldsymbol\epsilon_{0}} &= \bar{\mathbf{V}}_{\epsilon_{0}} \; \underline{\boldsymbol\epsilon_{0}} 

\end{align}
```



### The Sampler function

Given that we have priors and other sampling parameters from another function, and also some fixed matrices such as  $\mathbf{H}'\mathbf{H}$ and $\mathbf{H^{*}}_{\alpha}'\mathbf{H^{*}}_{\alpha}$.


We can sampling each of $\epsilon$ and $\epsilon_{0}$ from the following function






```{r}
Sampling.epsilon.epsilonzero    = function(starting.values, priors){

  
  aux            = starting.values
  p              = length(aux$alpha)
  T              = nrow(aux$Y)
  
    
    # Start calculating (and updating parameters)
    ###########################

    epsilon.star        = rbind(aux$epsilon.zero, aux$epsilon)
    omega               = sigma.e.star%*%solve(HaHa)
    omega_11            = omega[1:p, 1:p]
    omega_12            = omega[1:P, (p+1):(p+T)]
    omega_21            = omega[(p+1):(p+T), 1:p]
    omega_22            = omega[(p+1):(p+T), (p+1):(p+T)]
    epsilon             = omega_21%*%solve(omega_11)%*%aux$epsilon.zero
    epsilon.zero        = omega_12%*%solve(omega_22)%*%aux$epsilon
    V.epsilon           = omega_22 - omega_21%*%solve(omega_11)%*%omega_12
    V.epsilon.zero      = omega_11 - omega_12%*%solve(omega_22)%*%omega_21
    
    # Sampling epsilon
    ###########################
    V.epsilon.post.inv     = solve(V.epsilon) + (1/aux$sigma.eta)*HH
    b.epsilon              = (1/aux$sigma.eta)*HH%*%(aux$Y- cumsum(i%*%priors$tau_0)) + 
                             solve(V.epsilon)%*%epsilon
    precision.L            = t(chol(V.epsilon.post.inv))
    epsilon.n              = rnorm(T)
    b.epsilon.tmp          = solve(precision.L, b.epsilon)
    epsilon.draw           = solve(t(precision.L), b.epsilon.tmp + epsilon.n)
    aux$epsilon            = epsilon.draw
    
    # Sampling epsilon.zero
    ###########################
    V.epsilon.zero.post.inv     = solve(V.epsilon.zero) + (1/sigma.zero)*diag(p)
    b.epsilon.zero              = epsilon.zero
    precision.L                 = t(chol(V.epsilon.zero.post.inv))
    epsilon.n                   = rnorm(p)
    b.epsilon.zero.tmp          = solve(precision.L, b.epsilon.zero)
    epsilon.zero.draw           = solve(t(precision.L), b.epsilon.zero.tmp + epsilon.n)
    aux$epsilon.zero            = epsilon.zero.draw
    
    # The draw
  output      = list(
    epsilon_draw = epsilon.draw, # Drawn epsilon values
    epsilon_zero_draw = epsilon.zero.draw # Drawn epsilon.zero values
  )
  return(output)
}
```

## Random walk with  time-varying drift parameter

## Student-t error terms

## Conditional heteroskedasticity

# Bayesian forecasting

## Predictive density

## Sampling from the predictive density




## References {.unnumbered}
