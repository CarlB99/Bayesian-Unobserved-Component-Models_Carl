---
title: "Bayesian Unobserved Component Models"
author:
  - name: Tomasz WoÅºniak
    url: https://github.com/donotdespair
    orcid: 0000-0003-2212-2378
    
  - name: Carl Buan
    url: https://github.com/CarlB99
    
    execute:
  #echo: false
citation: 
  issued: 2024-05-01
  url: https://donotdespair.github.io/Bayesian-Unobserved-Component-Models/
  doi: 10.26188/25814617
bibliography: references.bib
---

> **Abstract.** We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. 
> The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. 
> We scrutinise Bayesian forecasting and sampling from the predictive density.
>
> **Keywords.** Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling

# Unobserved component models

Unobserved Component (UC) models are a popular class of models in macroeconometrics that use the state-space representation for unit-root nonstationary time series. 
The simple formulation of the model equations decomposing the series into a non-stationary and stationary component facilitates economic interpretations and good forecasting performance.

# A simple local-level model

The model is set for a univariate time series whose observation at time
$t$ is denoted by $y_t$. 
It decomposes the variable into a stochastic trend component, $\tau_t$, and a stationary error component, $\epsilon_t$. 
The former follows a Gaussian random walk process with the conditional variance $\sigma_\eta^2$, and the latter is zero-mean normally distributed with the variance $\sigma^2$.
These are expressed as the model equations:
\begin{align}
y_t &= \tau_t + \epsilon_t,\\
\tau_t &= \tau_{t-1} + \eta_t,\\
\epsilon_t &\sim\mathcal{N}\left(0, \sigma^2\right),\\
\eta_t &\sim\mathcal{N}\left(0, \sigma_\eta^2\right),
\end{align}
where the initial condition $\tau_0$ is a parameter of the model.

## Matrix notation for the model

To simplify the notation and the derivations introduce matrix notation for the model. Let $T$ be the available sample size for the variable $y$. 
Define a $T$-vector of zeros, $\mathbf{0}_T$, and of ones, $\boldsymbol\imath_T$, the identity matrix of order $T$, $\mathbf{I}_T$, as well as $T\times1$ vectors:
\begin{align}
\mathbf{y} = \begin{bmatrix} y_1\\ \vdots\\ y_T \end{bmatrix},\quad
\boldsymbol\tau = \begin{bmatrix} \tau_1\\ \vdots\\ \tau_T \end{bmatrix},\quad
\boldsymbol\epsilon = \begin{bmatrix} \epsilon_1\\ \vdots\\ \epsilon_T \end{bmatrix},\quad
\boldsymbol\eta = \begin{bmatrix} \eta_1\\ \vdots\\ \eta_T \end{bmatrix},\qquad
\mathbf{i} = \begin{bmatrix} 1\\0\\ \vdots\\ 0 \end{bmatrix},
\end{align}
and a $T\times T$ matrix $\mathbf{H}$ with the elements:
\begin{align}
\mathbf{H} = \begin{bmatrix}
1 & 0 & \cdots & 0 & 0\\
-1 & 1 & \cdots & 0 & 0\\
0 & -1 & \cdots & 0 & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
0 & 0 & \cdots & 1 & 0\\
0 & 0 & \cdots & -1 & 1
\end{bmatrix}.
\end{align}

Then the model can be written in a concise notation as:
\begin{align}
\mathbf{y} &= \mathbf{\tau} + \boldsymbol\epsilon,\\
\mathbf{H}\boldsymbol\tau &= \mathbf{i} \tau_0 + \boldsymbol\eta,\\
\boldsymbol\epsilon &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma^2\mathbf{I}_T\right),\\
\boldsymbol\eta &\sim\mathcal{N}\left(\mathbf{0}_T, \sigma_\eta^2\mathbf{I}_T\right).
\end{align}

## Likelihood function

The model equations imply the predictive density of the data vector
$\mathbf{y}$. To see this, consider the model equation as a linear
transformation of a normal vector $\boldsymbol\epsilon$. Therefore, the data vector follows a multivariate normal distribution given by:
\begin{align}
\mathbf{y}\mid \boldsymbol\tau, \sigma^2 &\sim\mathcal{N}_T\left(\boldsymbol\tau, \sigma^2\mathbf{I}_T\right).
\end{align}

This distribution determines the shape of the likelihood function that
is defined as the sampling data density: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y})\equiv p\left(\mathbf{y}\mid\boldsymbol\tau, \sigma^2 \right).
\end{align}

The likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of $\mathbf{y}$, is considered a function of parameters $\boldsymbol\tau$ and $\sigma^2$ is given by: 
\begin{align}
L(\boldsymbol\tau,\sigma^2|\mathbf{y}) = 
(2\pi)^{-\frac{T}{2}}\left(\sigma^2\right)^{-\frac{T}{2}}\exp\left\{-\frac{1}{2}\frac{1}{\sigma^2}(\mathbf{y} - \boldsymbol\tau)'(\mathbf{y} - \boldsymbol\tau)\right\}.
\end{align}

## Prior distributions

# Bayesian estimation

## Gibbs sampler

## Simulation smoother and precision sampler

## Analytical solution for a joint posterior

# Hierarchical modeling

## Estimating gamma error term variance prior scale


## Estimating inverted-gamma 2 error term variance prior scale

## Estimating the initial condition prior scale

## Student-t prior for the trend component

## Estimating Student-t degrees of freedom parameter

## Laplace prior for the trend component

# Model extensions

## Estimation of autoregressive parameters for cycle component

We consider an extension of the model by a zero-mean autoregressive equation for $\epsilon_{t}$:

\begin{align}
\epsilon_{t}=\alpha_{1}\epsilon_{t-1}=...+\alpha_{p}\epsilon_{t-p}+e_{t}
\end{align}

Where:

\begin{align}
e_{t}\sim\mathcal{N}\left(0,\sigma_{\alpha}^{2}\right)
\end{align}

Then the prior for the autoregressive parameter vector $\alpha=\left(\alpha_{1},...,\alpha_{p}\right)'$:

\begin{align}
\alpha|\sigma_{\alpha}^{2}\sim\mathcal{N}_{p}\left(0_{p},\sigma_{\alpha}^{2}I_{p}\right)
\end{align}

and:

\begin{align}
\sigma_{\alpha}^{2}\sim IG2\left(s_{\alpha},\nu_{\alpha}\right)
\end{align}

The prior distribution for $\alpha$ is then:

\begin{align}
\alpha\sim\mathcal{N}_{p}\left(\underline{\alpha},\underline{V}_{\alpha}\right)\mathcal{I}\left(\alpha\in A\right)\propto\exp\left(-\frac{1}{2}\left(\alpha-\underline{\alpha}\right)'\underline{V}_{\alpha}^{-1}\left(\alpha-\underline{\alpha}\right)\right)\mathcal{I}\left(\alpha\in A\right)
\end{align}

Where $\alpha\in A$ denotes the set of parameters for $\alpha$ in which stationarity holds:

\begin{align}
\mathcal{I}\left(\alpha\in A\right)=\begin{cases}
1 & \text{if }\alpha\in A\\
0 & \text{otherwise}
\end{cases}
\end{align}

Using from the simple unobserved component model:

\begin{align}
e=\epsilon-X_{\epsilon}\alpha
\end{align}

Then we have the likelihood:
\begin{align}
L\left(\alpha|y,\epsilon,\sigma_{\alpha}^{2}\right)\propto\exp\left(-\frac{1}{2}\sigma_{\alpha}^{2}\left(X_{\epsilon}\alpha-\epsilon\right)'\underline{V}_{\alpha}^{-1}\left(X_{\epsilon}\alpha-\epsilon\right)\right)
\end{align}

So the full-conditional posterior distribution for $\alpha$ is then given by:
\begin{align}
p\left(\alpha|y,\epsilon,\sigma_{\alpha}^{2}\right)&\propto L\left(\alpha|y,\epsilon,\sigma_{\alpha}^{2}\right)p\left(\alpha\right)\mathcal{I}\left(\alpha\in A\right)\\&=\exp\left(-\frac{1}{2}\sigma_{\alpha}^{2}\left(X_{\epsilon}\alpha-\epsilon\right)'\underline{V}_{\alpha}^{-1}\left(X_{\epsilon}\alpha-\epsilon\right)\right)\exp\left(-\frac{1}{2}\left(\alpha-\underline{\alpha}\right)'\underline{V}_{\alpha}^{-1}\left(\alpha-\underline{\alpha}\right)\right)\mathcal{I}\left(\alpha\in A\right)\\&=\exp\left(-\frac{1}{2}\left(\sigma_{\alpha}^{2}\left(X_{\epsilon}\alpha-\epsilon\right)'\underline{V}_{\alpha}^{-1}\left(X_{\epsilon}\alpha-\epsilon\right)+\left(\alpha-\underline{\alpha}\right)'\underline{V}_{\alpha}^{-1}\left(\alpha-\underline{\alpha}\right)\right)\right)\mathcal{I}\left(\alpha\in A\right)\\&=...\\&=\exp\left(-\frac{1}{2}\left(\alpha'\overline{V}_{\alpha}\alpha-2\alpha'\overline{V}_{\alpha}^{-1}\overline{\alpha}+...\right)\right)\mathcal{I}\left(\alpha\in A\right)
\end{align}

Hence we have the full-conditional posterior as:

\begin{align}
p\left(\alpha|y,\epsilon,\sigma_{\alpha}^{2}\right)&=\mathcal{N}_{p}\left(\overline{\alpha},\overline{V}_{\alpha}\right)\mathcal{I}\left(\alpha\in A\right)\\\overline{V}_{\alpha}&=\left[\sigma_{\alpha}^{-2}X_{\epsilon}'X_{\epsilon}+\underline{V}_{\alpha}^{-1}\right]^{-1}\\\overline{\alpha}&=\overline{V}_{\alpha}\left[\sigma_{\alpha}^{-2}X_{\epsilon}'\epsilon+\underline{V}_{\alpha}^{-1}\underline{\alpha}\right]
\end{align}

Now doing the same for $\sigma_{\alpha}^{2}$, where the prior and likelihood distribution is then:

\begin{align}
p\left(\sigma_{\alpha}^{2}\right)&\propto\left(\sigma_{\alpha}^{2}\right)^{-\frac{\nu_{\alpha}+2}{2}}\exp\left(-\frac{1}{2}\frac{s_{\alpha}}{\sigma_{\alpha}^{2}}\right)\\L\left(\sigma_{\alpha}^{2}|y,\epsilon,\alpha\right)&\propto\left(\sigma_{\alpha}^{2}\right)^{-\frac{T}{2}}\exp\left(-\frac{1}{2}\sigma_{\alpha}^{-2}\left(X_{\epsilon}\alpha-\epsilon\right)'\left(X_{\epsilon}\alpha-\epsilon\right)\right)
\end{align}

So the full-conditional posterior distribution for $\sigma_{\alpha}^{2}$ is then given by:
\begin{align}
p\left(\sigma_{\alpha}^{2}|y,\epsilon,\alpha\right)&\propto L\left(\sigma_{\alpha}^{2}|y,\epsilon,\alpha\right)p\left(\sigma_{\alpha}^{2}\right)\\&=\left(\sigma_{\alpha}^{2}\right)^{-\frac{T}{2}}\exp\left(-\frac{1}{2}\sigma_{\alpha}^{-2}\left(X_{\epsilon}\alpha-\epsilon\right)'\left(X_{\epsilon}\alpha-\epsilon\right)\right)\left(\sigma_{\alpha}^{2}\right)^{-\frac{\nu_{\alpha}+2}{2}}\exp\left(-\frac{1}{2}\frac{s_{\alpha}}{\sigma_{\alpha}^{2}}\right)\\&=\left(\sigma_{\alpha}^{2}\right)^{-\frac{T+\nu_{\alpha}+2}{2}}\exp\left(-\frac{1}{2}\sigma_{\alpha}^{-2}\left(\left(X_{\epsilon}\alpha-\epsilon\right)'\left(X_{\epsilon}\alpha-\epsilon\right)+s_{\alpha}\right)\right)\\&=\left(\sigma_{\alpha}^{2}\right)^{-\frac{\overline{\nu}_{\alpha}+2}{2}}\exp\left(-\frac{1}{2}\frac{\overline{s}_{\alpha}}{\sigma_{\alpha}^{2}}\right)
\end{align}

Hence we have the full-conditional posterior as:
\begin{align}
p\left(\alpha|y,\epsilon,\sigma_{\alpha}^{2}\right)&=\mathcal{IG}2\left(\overline{s}_{\alpha},\overline{\nu}_{\alpha}\right)\\\overline{s}_{\alpha}&=\left(X_{\epsilon}\alpha-\epsilon\right)'\left(X_{\epsilon}\alpha-\epsilon\right)+s_{\alpha}\\\overline{\nu}&=\nu_{\alpha}+T
\end{align}


```{r}
#| echo: true
#| message: false
#| warning: false
#| eval: false

# Assuming we have set a list of priors and we have the data

# Bayesian UC function

UC.Gibbs = function(Y,X,priors,S,initial.sigma2){
  # Gibbs sampler for a simple unobserved component model
  
  posterior = matrix(NA, sum(S), 2)
  colnames(posterior) = c("alpha", "sigma2")
  
  draw  = c(NA, initial.sigma2)
  names(draw) = c("alpha", "sigma2")
  
  for (s in 1:sum(S)){
    # full conditional posterior distribution alpha
    V.alpha.bar = 1/((1/priors[2]) + (1/draw[2])* as.numeric(t(X)%*%X))
    alpha.bar        = V.alpha.bar*( priors[1]/priors[2] + (1/draw[2])*as.numeric(t(X)%*%epsilon) )
    draw[1]         = rnorm(1, mean=beta.bar, sd=sqrt(sigma2.alpha.bar))
    
    # full conditional posterior distribution sigma2
    s.bar           = priors[3] + as.numeric(t(draw[1]*X-epsilon)%*%(draw[1]*X-epsilon))
    nu.bar          = priors[4] + nrow(Y)
    draw[2]         = s.bar/rchisq(1, nu.bar)
    
    posterior[s,]   = draw
  }
  
  posterior   = posterior[-(1:S[1]),]
  return(posterior)
}

```

## Autoregressive cycle component

## Random walk with  time-varying drift parameter

## Student-t error terms

## Conditional heteroskedasticity

# Bayesian forecasting

## Predictive density

## Sampling from the predictive density




## References {.unnumbered}
